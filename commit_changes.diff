commit b1fa5833bb1facb8f9cdd69a4594035a7423a9f2
Author: Nicolas <aira.nicolas94@gmail.com>
Date:   Fri Jan 24 14:24:34 2025 -0300

    Added sub-graph (window) level embeddings

diff --git a/predict_embedding.py b/predict_embedding.py
index 3288b91..c823309 100755
--- a/predict_embedding.py
+++ b/predict_embedding.py
@@ -8,7 +8,7 @@ from tqdm import tqdm
 import argparse
 from src.model.gin_model import GINModel
 from src.model.siamese_model import SiameseResNetLSTM
-from src.utils import dotbracket_to_forgi_graph, forgi_graph_to_tensor, log_information, log_setup, pad_and_convert_to_contact_matrix, dotbracket_to_graph, graph_to_tensor
+from src.utils import dotbracket_to_forgi_graph, forgi_graph_to_tensor, log_information, log_setup, pad_and_convert_to_contact_matrix, dotbracket_to_graph, graph_to_tensor, generate_slices
 import os
 import subprocess
 from pathlib import Path
@@ -78,9 +78,12 @@ def get_siamese_embedding(model, structure, max_len, device='cpu'):
 # Function to get embedding from graph
 
 
-def get_gin_embedding(model, graph_encoding, structure, device):
+def get_gin_embedding(model, graph_encoding, structure, device, L=None, keep_paired_neighbors=False):
+    # Convert to graph
     if graph_encoding == "standard":
         graph = dotbracket_to_graph(structure)
+        # Ensure nodes are 0-indexed and sorted
+        nodes = sorted(graph.nodes())
         tg = graph_to_tensor(graph)
     elif graph_encoding == "forgi":
         graph = dotbracket_to_forgi_graph(structure)
@@ -88,10 +91,47 @@ def get_gin_embedding(model, graph_encoding, structure, device):
 
     tg.to(device)
     model.eval()
+    
     with torch.no_grad():
-        embedding = model.forward_once(tg)
-    return ','.join(f'{x:.6f}' for x in embedding.cpu().numpy().flatten())
-
+        # Get node embeddings from trained model
+        node_embs = model.get_node_embeddings(tg)
+        
+        # Handle subgraph case
+        if L is not None:
+            # Get sorted nodes from the original graph
+            sorted_nodes = sorted(graph.nodes())
+            n = len(sorted_nodes)
+            
+            if n < L:
+                return [(-1, "")]
+            
+            embeddings = []
+            slices = generate_slices(graph, L, keep_paired_neighbors)
+            
+            for start_idx, subgraph_H in slices:
+                # Map subgraph nodes to their positions in the sorted list
+                subgraph_nodes = sorted(subgraph_H.nodes())
+                node_indices = [sorted_nodes.index(node) for node in subgraph_nodes]
+                
+                if not node_indices:
+                    continue
+                
+                # Get embeddings using tensor indices
+                sub_embs = node_embs[node_indices]
+                
+                # Pool and project
+                batch = torch.zeros(len(sub_embs), dtype=torch.long, device=device)
+                pooled = model.pooling(sub_embs, batch)
+                sub_embedding = model.fc(pooled)
+                
+                embedding_str = ','.join(f'{x:.6f}' for x in sub_embedding.cpu().numpy().flatten())
+                embeddings.append((start_idx, embedding_str))
+            
+            return embeddings if embeddings else [(-1, "")]
+        
+        # Handle whole graph case
+        whole_graph_embedding = model.forward_once(tg)
+        return [(None, ','.join(f'{x:.6f}' for x in whole_graph_embedding.cpu().numpy().flatten()))]
 
 # Function to validate dot-bracket structure
 def validate_structure(structure):
@@ -104,19 +144,26 @@ def validate_structure(structure):
 
 # Function to generate embeddings for a single row
 def generate_embedding_for_row(args):
-    idx, row, model, model_type, structure_column, max_len, device, graph_encoding = args
+    idx, row, model, model_type, structure_column, max_len, device, graph_encoding, L, keep_paired_neighbors = args
     
     structure = row[structure_column]
     validate_structure(structure)
+    
     if model_type == "siamese":
         embedding = get_siamese_embedding(model, structure, max_len, device=device)
+        return [(idx, None, embedding)]
+    
     elif "gin" in model_type:
-        embedding = get_gin_embedding(model, graph_encoding, structure, device)
-    return idx, embedding
+        embeddings = get_gin_embedding(
+            model, graph_encoding, structure, device, 
+            L=L, keep_paired_neighbors=keep_paired_neighbors
+        )
+        return [(idx, start, emb) for (start, emb) in embeddings]
+    
+    return []
 
 # Main function to generate embeddings from CSV or TSV
 
-
 def generate_embeddings(
         input_df,
         output_path,
@@ -130,40 +177,73 @@ def generate_embeddings(
         gin_layers=1,
         hidden_dim=256,
         output_dim=128,
+        subgraphs=False,
+        L=None,
+        keep_paired_neighbors=False,
         num_workers=4
 ):
-    # Load the trained model once
+    # Validate subgraph parameters
+    if subgraphs:
+        if L is None:
+            raise ValueError("Window length (L) must be specified when generating subgraphs")
+        if L < 1:
+            raise ValueError("Window length (L) must be a positive integer")
+
+    # Reset index to ensure positional access works correctly
+    input_df = input_df.reset_index(drop=True)
+
+    # Load the trained model
     model = load_trained_model(
         model_path,
         model_type,
         graph_encoding,
         device=device,
-        gin_layers= gin_layers,
+        gin_layers=gin_layers,
         hidden_dim=hidden_dim,
         output_dim=output_dim
     )
 
-    # Initialize list for storing embeddings
-    embeddings = [None] * len(input_df)
-
     # Prepare arguments for multiprocessing
-    args_list = [(idx, row, model, model_type, structure_column, max_len, device, graph_encoding) for idx, row in input_df.iterrows()]
-
-    # Use multiprocessing to generate embeddings
+    args_list = [
+        (idx, row, model, model_type, structure_column, max_len, device, 
+         graph_encoding, L if subgraphs else None, keep_paired_neighbors)
+        for idx, row in input_df.iterrows()
+    ]
+
+    # Process embeddings
+    results = []
     with Pool(num_workers) as pool:
-        for idx, embedding in tqdm(pool.imap_unordered(generate_embedding_for_row, args_list), total=len(input_df), desc="Processing Embeddings"):
-            embeddings[idx] = embedding
-
-    # Add the embeddings to the DataFrame
-    input_df['embedding_vector'] = embeddings
+        for result in tqdm(pool.imap_unordered(generate_embedding_for_row, args_list),
+                          total=len(input_df),
+                          desc="Processing Embeddings"):
+            results.extend(result)
+
+    # Create new DataFrame with expanded subgraph embeddings
+    new_rows = []
+    for original_idx, window_start, embedding in results:
+        if original_idx >= len(input_df):
+            raise ValueError(f"Invalid original index {original_idx} in results")
+            
+        original_row = input_df.iloc[original_idx].to_dict()
+        original_row.update({
+            'window_start': window_start if window_start is not None else -1,
+            'embedding_vector': embedding
+        })
+        new_rows.append(original_row)
+
+    output_df = pd.DataFrame(new_rows)
 
     # Save the output TSV
-    input_df.to_csv(output_path, sep='\t', index=False)
+    output_df.to_csv(output_path, sep='\t', index=False)
     print(f"Embeddings saved to {output_path}")
-    save_log = {
-        "Embeddings saved path": output_path
-    }
-    log_information(log_path, save_log)
+    
+    log_information(log_path, {
+        "Embeddings saved path": output_path,
+        "Total embeddings generated": len(output_df),
+        "Subgraph mode": subgraphs,
+        "Window length": L if subgraphs else "N/A",
+        "Keep paired neighbors": keep_paired_neighbors if subgraphs else "N/A"
+    })
 
 def read_input_data(input, samples, structure_column_num, header):
     delimiter = '\t' if input.endswith('.tsv') else ','
@@ -230,6 +310,12 @@ if __name__ == "__main__":
                         help='Specify whether the input CSV file has a header (default: True). Use "True" or "False".')
     parser.add_argument('--hidden_dim', type=int, default=256, help='Hidden dimension size for the model.')
     parser.add_argument('--output_dim', type=int, default=128, help='Output embedding size for the GIN model (ignored for siamese).')
+    parser.add_argument('--subgraphs', action='store_true', 
+                    help='Generate subgraph embeddings instead of whole graph embeddings.')
+    parser.add_argument('--L', type=int, 
+                        help='Window length for subgraph generation. Required if --subgraphs is set.')
+    parser.add_argument('--keep_paired_neighbors', action='store_true', 
+                        help='Include paired neighbors in subgraphs. Requires --subgraphs.')
     parser.add_argument('--device', type=str, default="cuda" if torch.cuda.is_available() else "cpu",
                         help='Device to run the model on (default: "cuda" if available, otherwise "cpu").')
     parser.add_argument('--num_workers', type=int, default=4, help='Number of worker processes to use for multiprocessing (default: 4).')
@@ -291,6 +377,9 @@ if __name__ == "__main__":
         gin_layers=args.gin_layers,
         hidden_dim=args.hidden_dim,
         output_dim=args.output_dim,
+        subgraphs=args.subgraphs,
+        L=args.L,
+        keep_paired_neighbors=args.keep_paired_neighbors,
         num_workers=args.num_workers
     )
 
diff --git a/src/model/gin_model.py b/src/model/gin_model.py
index 6654839..ba93fcf 100644
--- a/src/model/gin_model.py
+++ b/src/model/gin_model.py
@@ -39,23 +39,20 @@ class GINModel(nn.Module):
         #if pooling is Set2Set
         ## self.fc = nn.Linear(hidden_dim * 2, output_dim)  # *2 due to Set2Set doubling feature dim
 
-    def forward_once(self, data):
+    def get_node_embeddings(self, data):
         x, edge_index, batch = data.x, data.edge_index, data.batch
-
-        # Pass through GIN convolution layer
-        for i, conv in enumerate(self.convs):
+        for conv in self.convs:
             x = conv(x, edge_index)
-            #x = self.dropout(x)
-
-        # Pooling for graph-level embedding
-        x = self.pooling(x, batch)
-        #x = self.dropout(x)
+        return x
 
-        # Final projection to embedding space
-        embedding = self.fc(x)
-
-        return embedding
+    def pool_and_project(self, x, batch):
+        x_pooled = self.pooling(x, batch)
+        return self.fc(x_pooled)
 
+    def forward_once(self, data):
+        x = self.get_node_embeddings(data)
+        return self.pool_and_project(x, data.batch)
+    
     def forward(self, anchor, positive, negative):
         # Forward pass for each of the triplet inputs
         anchor_out = self.forward_once(anchor)
diff --git a/src/utils.py b/src/utils.py
index 5e411aa..feb4eea 100644
--- a/src/utils.py
+++ b/src/utils.py
@@ -140,14 +140,29 @@ def dotbracket_to_graph(dotbracket):
     return G
 
 
-def graph_to_tensor(g):
-    x = torch.Tensor([[0] if g.nodes[node]['label'] == 'unpaired' else [1] for node in g.nodes])
-    edge_index = torch.LongTensor(list(g.edges())).t().contiguous()
-
-    # Graph to Data object
-    data = Data(x=x, edge_index=edge_index)
-
-    return data
+def graph_to_tensor(G):
+    # Get nodes in sorted order to maintain index consistency
+    nodes = sorted(G.nodes())
+    node_features = []
+    for node in nodes:  # Now processing nodes in 0-based order
+        label = G.nodes[node]['label']
+        features = [1.0] if label == 'paired' else [0.0]
+        node_features.append(features)
+    
+    x = torch.tensor(node_features, dtype=torch.float)
+    
+    edge_indices = []
+    edge_attrs = []
+    for u, v, data in G.edges(data=True):
+        edge_indices.append([u, v])
+        edge_type = data['edge_type']
+        attr = [1.0, 0.0] if edge_type == 'adjacent' else [0.0, 1.0]
+        edge_attrs.append(attr)
+    
+    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()
+    edge_attr = torch.tensor(edge_attrs, dtype=torch.float)
+    
+    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)
 
 def dotbracket_to_forgi_graph(dotbracket):
     bulge_graph = fgb.BulgeGraph.from_dotbracket(dotbracket)
@@ -288,3 +303,34 @@ def log_information(log_path, info_dict, log_name = None, open_type='a', print_l
             f.write(to_log)
             if print_log:
                 print(to_log)
+
+def generate_slices(G, L, keep_paired_neighbors=True):
+    slices = []
+    nodes = sorted(G.nodes())
+    n = len(nodes)
+    
+    for start in range(n - L + 1):
+        window_nodes = list(range(start, start + L))
+        external_nodes = set()
+        
+        if keep_paired_neighbors:
+            for node in window_nodes:
+                for neighbor in G.neighbors(node):
+                    if G.edges[node, neighbor].get('edge_type') == 'base_pair' and neighbor not in window_nodes:
+                        external_nodes.add(neighbor)
+        
+        subgraph_nodes = set(window_nodes).union(external_nodes)
+        H = G.subgraph(subgraph_nodes).copy()
+        
+        if keep_paired_neighbors:
+            for external_node in external_nodes:
+                edges_to_remove = []
+                for neighbor in H.neighbors(external_node):
+                    if H.edges[external_node, neighbor].get('edge_type') == 'adjacent':
+                        edges_to_remove.append((external_node, neighbor))
+                for u, v in edges_to_remove:
+                    H.remove_edge(u, v)
+        
+        slices.append((start, H))
+    
+    return slices
